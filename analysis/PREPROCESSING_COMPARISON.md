# PREPROCESSING COMPARISON: Original DCASE vs Current Implementation

## Executive Summary

The current implementation significantly **refactors and improves** the original DCASE preprocessing pipeline. While the core audio feature extraction (WAV â†’ log-mel/PCEN) remains similar, there are **major architectural differences** in:

1. **Feature Storage Strategy**: Pre-computed `.npy` files vs. on-the-fly computation
2. **Segmentation Approach**: Random cropping vs. fixed-length padding
3. **Dataset Architecture**: Flat event-based vs. sequence-based dynamic arrays
4. **Configuration Management**: Hydra-based configs vs. YAML/Python dicts
5. **Normalization**: Global statistics vs. per-feature computation

---

## 1. CORE FEATURE EXTRACTION (SIMILAR)

### Original DCASE Approach
**Files:** `baselines/dcase2024_task5/src/datamodules/components/pcen.py`

```python
# Audio Loading & Feature Extraction
y, fs = librosa.load(audio_path, sr=22050)
mel_spec = librosa.feature.melspectrogram(
    y, sr=22050, n_fft=1024, hop_length=256, n_mels=128, fmax=11025
)
logmel = np.log(mel_spec + eps)
pcen = librosa.core.pcen(mel_spec, sr=22050)
features = pcen.T  # Transpose to (time, frequency)
```

### Current Implementation
**Files:** `preprocessing/preprocess.py`

```python
# Same core steps but modularized
def waveform_to_logmel(waveform, cfg):
    mel = librosa.feature.melspectrogram(
        y=waveform, sr=sr, n_fft=n_fft, hop_length=hop_length,
        n_mels=n_mels, fmin=fmin, fmax=fmax, power=2.0
    )
    logmel = np.log(mel + eps)
    return logmel.astype(np.float32)

def waveform_to_pcen(waveform, cfg):
    mel = librosa.feature.melspectrogram(...)
    pcen = librosa.pcen(mel, sr=sr, hop_length=hop_length)
    return pcen.astype(np.float32)
```

### Key Difference âœ“
- âœ“ **Current is cleaner**: Separate functions for each feature type
- âœ“ **Current supports both**: Log-mel OR PCEN (configurable)
- âœ“ **Current normalizes waveform**: `waveform = waveform / max(waveform)` before processing
- âœ“ **Same parameters**: sr=22050, n_fft=1024, hop_mel=256, n_mels=128

---

## 2. FEATURE STORAGE STRATEGY (MAJOR DIFFERENCE!!!!)

### Original DCASE Approach
**Files:** `baselines/dcase2024_task5/src/datamodules/components/feature_extract.py`

```
STRATEGY: Pre-compute and cache to HDF5
â”œâ”€ Compute PCEN for all audio files (one-time)
â”œâ”€ Store in HDF5 file structure
â”‚  â””â”€ hf['features'][file_index] = pcen_patch  # Shape: (17, 128)
â”‚  â””â”€ hf['labels'][file_index] = class_label
â”œâ”€ During training: load from HDF5 (disk â†’ RAM)
â””â”€ Pros: Fast training iteration (already computed)
        Cons: Huge disk space, inflexible
```

**Code Example:**
```python
# Original: Pre-compute and store
for audio_file in all_audio_files:
    y, sr = librosa.load(audio_file, sr=22050)
    pcen = librosa.core.pcen(mel_spec)
    
    # Store segment in HDF5
    hf['features'].resize((index + 1, 17, 128))
    hf['features'][index] = pcen_patch
    hf['labels'][index] = label
    
# During training: just load
feature = hf['features'][idx]  # Already precomputed
```

### Current Implementation
**Files:** `preprocessing/feature_export.py`, `preprocessing/preprocess.py`

```
STRATEGY: Pre-compute to .npy files, load on-demand
â”œâ”€ Export phase: for each WAV file
â”‚  â””â”€ Compute logmel/PCEN and save as:
â”‚     â”œâ”€ audio.wav â†’ audio_logmel.npy  (shape: 128, time_steps)
â”‚     â””â”€ audio.wav â†’ audio_pcen.npy    (shape: 128, time_steps)
â”œâ”€ During training: load .npy â†’ extract segment â†’ normalize
â”‚  â””â”€ Pros: Flexible, disk-efficient, configurable feature types
â”‚  â””â”€ Cons: Slower loading (disk I/O during training)
â””â”€ Key feature: Segment extraction happens at LOAD TIME
```

**Code Example:**
```python
# Current: Export features once
def export_features(cfg):
    for wav_path in all_wav_files:
        waveform, sr = load_audio(wav_path, cfg)
        logmel = waveform_to_logmel(waveform, cfg)
        pcen = waveform_to_pcen(waveform, cfg)
        
        np.save(wav_path.replace('.wav', '_logmel.npy'), logmel)
        np.save(wav_path.replace('.wav', '_pcen.npy'), pcen)

# During training: load and segment
def extract_logmel_segment(wav_path, start_time, end_time, cfg):
    waveform, sr = load_audio(wav_path, cfg)
    # Load entire feature file
    logmel = np.load(wav_path.replace('.wav', '_logmel.npy'))
    # Extract time segment
    start_frame = int(start_time * fps)
    end_frame = int(end_time * fps)
    segment = logmel[:, start_frame:end_frame]
    return segment
```

---

## 3. SEGMENTATION STRATEGY (MAJOR DIFFERENCE ðŸ”´)

### Original DCASE: Fixed-Length with Tiling
**Code from:** `baselines/dcase2024_task5/src/datamodules/components/dynamic_pcen_dataset.py`

```python
def select_segment(self, start, end, pcen, seg_len=17):
    """
    Extract fixed-length segment (17 frames)
    If shorter: TILE/REPEAT the segment
    If longer: random crop
    """
    start_frame = int(start * self.fps)
    end_frame = int(end * self.fps)
    duration_frames = end_frame - start_frame
    
    if duration_frames < seg_len:
        # SHORT: Repeat/tile the segment
        repeat_num = int(seg_len / duration_frames) + 1
        x = np.tile(pcen[:, start_frame:end_frame], (1, repeat_num))
        x = x[:, 0:seg_len]
        return x
    else:
        # LONG: Random crop
        rand_start = np.random.randint(0, duration_frames - seg_len + 1)
        return pcen[:, start_frame + rand_start : start_frame + rand_start + seg_len]
```

**Example:**
```
Annotation: 100-110 frames (10 frames, too short)
seg_len: 17 frames

Original DCASE:
  â”œâ”€ Tile twice: [100-110, 100-110, ...]
  â”œâ”€ Trim to 17: [100-110, 100-110, 100-106]
  â””â”€ Output shape: (128, 17)

Annotation: 100-200 frames (100 frames, too long)
  â”œâ”€ Random start: 50 (between 0 and 83)
  â”œâ”€ Extract: [100+50:100+50+17] = [150:167]
  â””â”€ Output shape: (128, 17)
```

### Current Implementation: Padding/Cropping (NEW APPROACH)
**Code from:** `preprocessing/dataset.py`, `preprocessing/preprocess.py`

```python
def crop_pad(t: torch.Tensor, T_max: int) -> torch.Tensor:
    """Crop or pad tensor to fixed time dimension."""
    T = t.shape[-1]
    if T > T_max:
        # Crop: take first T_max frames
        t = t[..., :T_max]
    elif T < T_max:
        # Pad: zero-padding at the end
        diff = T_max - T
        t = F.pad(t, (0, diff))
    return t

# In FewShotEpisodeDataset:
def extract_logmel_segment(wav_path, start_time, end_time, cfg):
    segment = waveform[start_sample:end_sample]
    
    # Pad short segments with zeros
    if min_duration is not None:
        min_samples = int(min_duration * sr)
        if len(segment) < min_samples:
            pad_width = min_samples - len(segment)
            segment = np.pad(segment, (0, pad_width), mode="constant")
```

**Example:**
```
Annotation: 100-110 frames (10 frames, shorter than max_frames)
max_frames: 256 (or T_max)

Current approach:
  â”œâ”€ Load segment: shape (128, 10)
  â”œâ”€ Pad with zeros: (128, 10) â†’ (128, 256)
  â””â”€ Output: (128, 256) with 246 zero-padded frames

Annotation: 100-200 frames (100 frames)
  â”œâ”€ Load segment: shape (128, 100)
  â”œâ”€ Crop: take first T_max=256? No, 100 < 256, so pad
  â””â”€ Output: (128, 256)
```

### Key Differences ðŸ”´

| Aspect | Original DCASE | Current |
|--------|----------------|---------|
| **Short segments** | TILE/REPEAT | ZERO-PAD at end |
| **Long segments** | RANDOM CROP | CROP from START |
| **Output shape** | Fixed (17, 128) | Variable (T_max, 128) |
| **Fixed length** | Yes (17 frames) | Flexible T_max (config) |
| **Artificial data?** | Yes (tiling introduces artifacts) | More realistic (padding) |
| **Why?** | Simple episodic training | Variable-length sequences |

---

## 4. DATASET ARCHITECTURE (MAJOR DIFFERENCE ðŸ”´)

### Original DCASE: Sequence-Based Dynamic Arrays
**Files:** `preprocessing/sequence_data/dynamic_pcen_dataset.py`

```
PrototypeDynamicArrayDataSet
â”œâ”€ Loads pre-computed PCEN features from .npy files
â”œâ”€ Each __getitem__ returns one SEGMENT (17 frames Ã— 128 bins)
â”œâ”€ Creates EPISODIC batches via IdentityBatchSampler
â”‚  â”œâ”€ Samples k_way=5 classes
â”‚  â”œâ”€ Samples n_shot=5 examples per class (support)
â”‚  â”œâ”€ Samples n_query=5 examples per class (query)
â”‚  â””â”€ Total batch: 50 examples
â””â”€ Trains with prototypical loss (metric learning)
```

**Code:**
```python
class PrototypeDynamicArrayDataSet(Dataset):
    def __getitem__(self, idx):
        class_name = self.classes[idx]
        segment = self.select_positive(class_name)  # (17, 128)
        return segment.astype(np.float32), self.classes2int[class_name]
```

### Current Implementation: Flat Event-Based OR Dynamic Arrays
**Files:** `preprocessing/dataset.py` (New), `preprocessing/datamodule.py`

**Two dataset paths:**

#### Path A: Flat DCASEEventDataset (NEW)
```
DCASEEventDataset
â”œâ”€ Loads all labeled segments (events) from CSV annotations
â”œâ”€ Each __getitem__ returns:
â”‚  â””â”€ Tensor shape (1, n_mels, T)  where T varies per example
â”œâ”€ Entire annotation as one example (NOT segmented into fixed 17 frames)
â”œâ”€ Wrapped in FewShotEpisodeDataset for episodic training
â”‚  â”œâ”€ Creates episodes on-the-fly
â”‚  â”œâ”€ Crops/pads to fixed T_max
â”‚  â””â”€ Returns support/query sets
â””â”€ More flexible: works with variable-length annotations
```

**Code:**
```python
class DCASEEventDataset(Dataset):
    def __getitem__(self, idx) -> Tuple[torch.Tensor, int]:
        ex = self.examples[idx]
        logmel = extract_logmel_segment(
            wav_path=ex.wav_path,
            start_time=ex.start_time,
            end_time=ex.end_time,
            cfg=self.cfg,
        )
        tensor = torch.from_numpy(logmel)[None, ...]  # (1, n_mels, T)
        label = ex.class_id
        return tensor, label

class FewShotEpisodeDataset(Dataset):
    def __getitem__(self, idx):
        # Sample k_way classes, n_shot support, n_query query
        # Crop/pad all to T_max
        # Return (support_x, support_y, query_x, query_y)
```

#### Path B: PrototypeDynamicArrayDataSet (ORIGINAL, still used)
```
Same as original DCASE
â”œâ”€ Dynamic pre-computed PCEN features
â”œâ”€ Fixed-length segments (17 frames)
â”œâ”€ Episodic batching via IdentityBatchSampler
â””â”€ Used when config specifies this dataset
```

---

## 5. CONFIGURATION MANAGEMENT (MODERATE DIFFERENCE)

### Original DCASE
**Files:** Various YAML files in `conf/` + Python dicts

```yaml
# Config scattered across multiple files
features:
  seg_len: 0.200          # Seconds
  hop_seg: 0.100          # Seconds
  sr: 22050
  n_fft: 1024
  hop_mel: 256
  n_mels: 128
  fmax: 11025

train_param:
  n_shot: 5
  k_way: 5
  negative_train_contrast: false
```

### Current Implementation
**Files:** `conf/config.yaml` (unified Hydra config)

```yaml
# Single unified config with Hydra
features:
  eps: 1e-8
  fmax: 11025
  fmin: 50
  sr: 22050
  n_fft: 1024
  n_mels: 128
  hop_mel: 256
  feature_types: logmel  # Can be "logmel" or "pcen" or "logmel@pcen"
  embedding_dim: 2048

train_param:
  seg_len: 0.2
  n_shot: 5
  k_way: 5
  adaptive_seg_len: false  # NEW: variable-length testing

annotations:
  min_duration: 0.2      # NEW: minimum segment duration
  max_frames: 256        # NEW: maximum frames for padding
  positive_label: "POS"
  class_name: "Class"
```

**Key additions:**
- âœ“ `feature_types`: Configurable (logmel, pcen, or both)
- âœ“ `adaptive_seg_len`: Variable-length testing
- âœ“ `max_frames`: Fixed padding dimension
- âœ“ `positive_label`: Flexible annotation parsing

---

## 6. NORMALIZATION & STATISTICS (MODERATE DIFFERENCE)

### Original DCASE
**Files:** `baselines/dcase2024_task5/src/datamodules/components/pcen.py`

```python
class Feature_Extractor:
    mean_std = {}  # Class variable
    
    def update_mean_std(self):
        """Compute global mean/std for each feature type"""
        for suffix in self.feature_types:
            features = []
            for audio_path in tqdm(self.files[:1000]):  # ~1000 files
                feature_path = audio_path.replace(".wav", f"_{suffix}.npy")
                features.append(np.load(feature_path).flatten())
            
            all_data = np.concatenate(features)
            mean = np.mean(all_data)  # Single value
            std = np.std(all_data)    # Single value
            Feature_Extractor.mean_std[suffix] = [mean, std]
    
    def extract_feature(self, audio_path, normalized=True):
        feat = np.load(...)
        if normalized:
            mean, std = Feature_Extractor.mean_std[suffix]
            feat = (feat - mean) / std  # Z-score
        return feat
```

**Stored statistics (for DCASE AudioMNIST):**
```
mean = 1.4421
std = 1.2201
```

### Current Implementation
**Status: NOT explicitly shown in current code**

- The current code loads features but **normalization approach is not clear**
- Likely uses the same global statistics from original
- OR computes on-the-fly during training (not shown in provided files)

---

## 7. DATA AUGMENTATION (NEW FEATURES)

### Original DCASE
```python
# Minimal augmentation
# - Tiling (handled as segment extraction)
# - Optional mixing with negative samples (commented out)
```

### Current Implementation
**New capabilities in `dynamic_pcen_dataset.py`:**

```python
# Optional negative contrast learning
if self.train_param.negative_train_contrast:
    segment_neg = self.select_negative(class_name)
    return (
        segment.astype(np.float32),
        segment_neg.astype(np.float32),
        self.classes2int[class_name] * 2,
        self.classes2int[class_name] * 2 + 1,
    )

# Optional adaptive segment length for testing
if self.train_param.adaptive_seg_len:
    self.data_test = PrototypeAdaSeglenBetterNegTestSetV2(...)
else:
    self.data_test = PrototypeTestSet(...)
```

**New augmentations:**
- âœ“ Negative contrast pairs
- âœ“ Adaptive segment length for evaluation
- âœ“ Better negative sampling strategy

---

## 8. ANNOTATION PARSING (NEW)

### Original DCASE
**Implicit CSV format handling**
```python
# Assumed specific CSV structure:
# Audiofilename, Starttime, Endtime, ...
```

### Current Implementation
**Explicit annotation service:**

**Files:** `preprocessing/ann_service.py`, `schemas/segment_example.py`

```python
class AnnotationService:
    """
    Handles multiple CSV formats:
    1. Multi-class CSVs with CLASS_x columns (POS/NEG/UNK)
    2. Single-class CSVs with 'Q' column (POS/UNK)
    3. Fallback: only Audiofilename/Starttime/Endtime (all positive)
    """
    
    def load_annotations(self, annotation_paths):
        # Parse different CSV formats
        # Create SegmentExample objects
        return examples  # List of SegmentExample

class SegmentExample:
    """Standardized annotation format"""
    wav_path: Path
    start_time: float
    end_time: float
    class_name: str
    class_id: int
```

**Benefits:**
- âœ“ Handles multiple annotation formats
- âœ“ Standardized data structures
- âœ“ Better error reporting

---

## 9. SUMMARY TABLE

| Aspect | Original DCASE | Current | Impact |
|--------|---|---|---|
| **Feature Extraction** | Log-mel + PCEN | Log-mel + PCEN (configurable) | âœ“ More flexible |
| **Feature Storage** | HDF5 (pre-computed) | .npy files (per-audio) | âœ“ More disk-efficient, flexible |
| **Segmentation** | Fixed 17 frames + tiling | Variable T_max + padding | ðŸ”´ Different training dynamics |
| **Short segment handling** | TILE/REPEAT | ZERO-PAD | ðŸ”´ Less artificial data |
| **Long segment handling** | RANDOM CROP | CROP from START | ðŸŸ¡ Different sampling |
| **Dataset class** | PrototypeDynamicArrayDataSet | DCASEEventDataset OR PrototypeDynamicArrayDataSet | âœ“ More options |
| **Episodic batching** | IdentityBatchSampler | Same IdentityBatchSampler | âœ“ Compatible |
| **Config system** | Scattered YAML + dicts | Hydra (unified) | âœ“ Better management |
| **Annotation parsing** | Implicit CSV | Explicit AnnotationService | âœ“ More robust |
| **Augmentation** | Minimal | Negative contrast, adaptive seg-len | âœ“ Enhanced |

---

## 10. WHICH DATASET ARE WE USING?

### For Training:
**Default:** `PrototypeDynamicArrayDataSet` (original DCASE)
```python
if self.train_param.use_validation_first_5:
    self.dataset = PrototypeDynamicArrayDataSetWithEval(...)
else:
    self.dataset = PrototypeDynamicArrayDataSet(...)  # Default
```

**Optional:** `DCASEEventDataset + FewShotEpisodeDataset` (new flat approach)
```python
# Create flat dataset
base_dataset = DCASEEventDataset(annotations=[...], cfg=cfg)
# Wrap for episodic training
episode_dataset = FewShotEpisodeDataset(base_dataset, cfg)
```

### For Validation:
**Primary:** `PrototypeDynamicArrayDataSetVal`
```python
self.val_dataset = PrototypeDynamicArrayDataSetVal(...)
```

### For Testing:
**Default:** `PrototypeTestSet`
```python
if self.train_param.adaptive_seg_len:
    self.data_test = PrototypeAdaSeglenBetterNegTestSetV2(...)
else:
    self.data_test = PrototypeTestSet(...)
```

---

## 11. KEY ARCHITECTURAL CHANGES

### 1. **Modularization** âœ“
- Original: Monolithic feature extraction in one class
- Current: Separated into `load_audio()`, `waveform_to_logmel()`, `waveform_to_pcen()`, `extract_logmel_segment()`

### 2. **Flexibility** âœ“
- Original: Fixed to PCEN, HDF5 storage, 17-frame segments
- Current: Configurable features, .npy storage, variable-length support

### 3. **Dual-path support** âœ“
- Original: Single PrototypeDynamicArrayDataSet
- Current: Can use either original sequence-based OR new flat event-based

### 4. **Better config management** âœ“
- Original: Multiple config files scattered
- Current: Unified Hydra configuration

### 5. **Production-ready annotations** âœ“
- Original: Assumes specific CSV format
- Current: Robust AnnotationService handling multiple formats

---

## 12. POTENTIAL ISSUES & CONSIDERATIONS

### Issue 1: Padding vs. Tiling
**Original (tiling):** Artificially extends short events by repeating them
```
Short event: [A, B, C] â†’ [A, B, C, A, B, C, A, B, C, ...]
```

**Current (padding):** Extends with zeros
```
Short event: [A, B, C] â†’ [A, B, C, 0, 0, 0, 0, ...]
```

**Impact:** Model trained on tiled data may not generalize well to padded data and vice versa. **This is a breaking change** if you're reusing pre-trained weights.

### Issue 2: Segment Extraction Timing
**Original:** Segments extracted during preprocessing â†’ fixed 17 frames
**Current:** Can extract at load time â†’ variable lengths possible

**Impact:** Current is more flexible but requires managing variable-length sequences in the model.

### Issue 3: Feature Storage Size
**Original HDF5:**
```
1000 audio files Ã— 430 timesteps Ã— 128 bins Ã— 4 bytes (float32)
= ~220 MB HDF5 file (with compression)
```

**Current .npy files:**
```
1000 separate files Ã— 128 Ã— T frames Ã— 4 bytes
= ~220 MB total (more fragmented)
```

**Impact:** Current is slightly less efficient for disk I/O but more flexible.

---

## 13. RECOMMENDED APPROACH

### Use `PrototypeDynamicArrayDataSet` (Current Default) if:
- âœ“ You want to match original DCASE behavior exactly
- âœ“ Pre-computed features work for your setup
- âœ“ Fixed-length 17-frame segments are sufficient
- âœ“ You're continuing from a pre-trained model

### Use `DCASEEventDataset + FewShotEpisodeDataset` if:
- âœ“ You have variable-length annotations
- âœ“ You want more flexible segment handling
- âœ“ You're starting fresh (no pre-trained weights)
- âœ“ You prefer padding over tiling
- âœ“ You want better code modularity

---

## 14. IMPLEMENTATION PRIORITY

To fully understand what's different:

1. **Check config.yaml** â†’ What feature_types are we using? (logmel vs pcen)
2. **Check datamodule.py** â†’ Which dataset class is instantiated?
3. **Check main.py** â†’ How is preprocessing called?
4. **Check archs/*/lightning_module.py** â†’ How does the model handle variable-length inputs?

